---
title: "Lena-Pred Pitch Extraction Pipeline"
output: 
  html_document:
    code_folding: hide
---

```{r, include = F}
knitr::opts_chunk$set(echo = T, warning = F, message = F,
                      fig.asp = 0.8, fig.width = 8,
                      fig.align = 'center', out.width = "80%")
```

## Setup and Overview

This code takes raw audio and converts it to a categorical time series representing the unfolding of pitch information. The goal is to create a feature set that is suited for a categorical time series prediction model. We want to create a series of states that we can then use to predict the next state in a sequence, taking into account long-term dependencies.

The data processing pipeline follows Rasanen et al. (2018):

* Extract F0 pitch contour from raw audio
* Interpolate F0 across unvoiced regions of the signal using loess model
* Log transform and z-score normalize F0
* Segement the time series into 100 millisecond bins
* Fit a second order polynomial to the F0 curve in each time bin
* Use k-means clustering algorithm to classify F0 shapes into categories
* Vectorize the sequence data for training LSTM 

```{r libraries, include = F}
library(soundgen); library(tidyverse); library(here)
source(here("code/00_helper_functions/pitch_helpers.R"))
source(here("code/00_helper_functions/lstm-helpers.R"))
source(here("code/00_helper_functions/plot_theme.R"))
source(here("code/00_config/lena-pred-pitch-config.R"))
```

## Extract pitch contours

Extract pitch contours for each .wav file. We use our own `get_pitch_contour()` function, which gets mapped over each .wav file. 

```{r extract pitch estimates, cache = T}
d <- files_to_analyze %>% map_df(get_pitch_contour, pitch_detect_config)
```

Find first and last voiced region and remove time values outside. This ensures that we aren't extrapolating outside the range of the data with our interpolation step.

```{r filter first and last voiced}
batch_filter_voiced(d) -> d
```

Plot a sample of eight pitch contours, four from each dataset and and speech register.

```{r plot sample pitch contours}
segs_to_plot = d %>% 
  distinct(dataset, seg_id, speech_register, path_to_wav) %>% 
  group_by(dataset, speech_register) %>% 
  sample_n(2)

d %>% 
  filter(seg_id %in% segs_to_plot$seg_id) %>% 
  ggplot(aes(x = time, y = pitch, color = speech_register)) +
  geom_point(size = 2) +
  labs(x = "Time (ms)", y = "Pitch ") +
  lims(y = c(pitch_detect_config$pitch_min, pitch_detect_config$pitch_max)) +
  facet_wrap(dataset + speech_register~seg_id, 
             #scales = "free_x", 
             ncol = 4) +
  theme(legend.position = 'top') +
  ggthemes::scale_color_ptol() 
```

Looks like we can extract pitch estimates from audio files in both datasets. 

### Interpolate pitch contour over unvoiced regions between pitch estimates

Create a blacklist of segment ids with too few pitch estimates to do any kind of reliable interpolation using the loess. Here we choose 20 pitch samples, which is 500 ms of pitch data. 

```{r}
seg_id_blacklist <- flag_too_few_pitch(d, loess_config$min_n_samples_loess) 

batch_interpolate(d, loess_config) -> d_interp  

# log transform and Z-score
d_interp %>% 
  mutate(log_pitch = log(pitch_interpolated), 
         z_log_pitch = scale(log_pitch)) -> d_interp
```

Let's plot the originial pitch estimates (points) with our interpolated pitch contours (black lines) to sanity check the interpolation step. 

```{r}
d_interp %>% 
  filter(seg_id %in% segs_to_plot$seg_id) %>% 
  ggplot(aes(x = time, y = pitch_interpolated)) +
  geom_line(size = 2, color = "grey20") +
  geom_point(data = filter(d, seg_id %in% segs_to_plot$seg_id), 
             aes(time, pitch, color = speech_register), 
             size = 2, 
             alpha = 0.7) +
  labs(x = "Time (ms)", y = "Pitch") +
  lims(y = c(pitch_detect_config$pitch_min, pitch_detect_config$pitch_max)) +
  facet_wrap(dataset + speech_register~seg_id, 
             #scales = "free_x", 
             ncol = 4) +
  theme(legend.position = 'top') +
  ggthemes::scale_color_ptol() 
```

These curves look reasonable to me, but the `span` parameter, which controls the wiggliness of the loess, is a free parameter that we should experiment with.

### Temporal segmentation 

Divide each audio clip into fixed frame 100ms segments.

```{r create time bins}
d_interp %>% 
  create_time_bins(bin_width = time_filter_config$time_bin_width) %>% 
  get_time_in_bin(sample_rate = loess_config$preds_sample_rate) %>% 
  relabel_bins() -> d_interp

# Remove 100 ms segments with fewer than the min number of samples in each bin.
d_interp %>% filter(n_bins_in_seg == time_filter_config$min_samples_bin) -> d_interp
```

Make a plot to sanity check temporal segmentation step where we color each point based on its 100 ms time bin.

```{r plot time bins}
one_seg_to_plot <- sample(segs_to_plot$seg_id, size = 1)

d_interp %>% 
  filter(seg_id %in% one_seg_to_plot) %>% 
  ggplot(aes(x = time, y = z_log_pitch, color = time_bin)) +
  geom_point(size = 1) +
  guides(color = F) +
  labs(x = "Time (ms)", y = "Normalized Log(Pitch)")  +
  facet_wrap(~seg_id) 
```

From the plot, it looks like the temporal segmentation step is working. 

### Fit second-order polynomial in each time bin

We pass the time and normalized log transformed pitch values and get out the coefficients of a second-order polynomial function for each time bin (vector with length 3). We hold onto the segment_id, dataset, speech register, and time_bin as metadata. 

```{r fit polynomial to time bins}
d_interp %>% 
  group_by(seg_id, dataset, speech_register, time_bin_id) %>% 
  nest() -> d_by_bin

# fit polynomial and make predictions based on those fits
d_by_bin %>% 
  mutate(poly_coefs = map(data, fit_poly, poly_fit_config$degree_poly)) %>% 
  mutate(poly_preds = map(poly_coefs, predict_poly)) -> d_by_bin
```

### K-means clustering on poly coefs

We pass a 2 x 2 matrix of coefficient values for each 100 ms segment of the pitch contour and get back  a cluster assignment for each segment.

```{r get clusters}
d_coefs <- unnest(d_by_bin, poly_coefs, .drop = T) 
d_coefs %>% get_cluster_assignments(k = poly_fit_config$n_q_shapes, scale_coefs = T) -> d_final
```

We can plot the distribution of cluster assignments for each dataset. 

```{r}
plot_clusters_scatter(d_final$d_clusters)
```

```{r plot pitch shapes, include = F}
plot_sample_pitch_shapes(d_final$d_clusters, frac_sample = 0.1)
```

The top panel shows the polynomial shape for the center of each cluster generated by the kmeans clustering. The bottom panel shows a reconstructed pitch contour by plotting the 2nd order polynomial for each 100 ms time bin (bottom row) alongside the interpolated pitch contour (top row). The number displayed in each time bin facet represents the cluster assignment for that pitch shape based on the kmeans step. 

```{r, out.width="60%", fig.width=4}
shapes_plot <- plot_cluster_shapes(d_final$centers, scaled = TRUE)
shapes_plot
```

```{r plot reconstructed pitch contour}
# add cluster assignments to display on plot
d_by_bin %>% left_join(select(d_final$d_clusters, seg_id, time_bin_id, cluster)) -> d_by_bin

# plot
recontruct_plot <- plot_reconstructed_pitch(one_seg_to_plot, df_raw = d_interp, df_preds = d_by_bin) 
recontruct_plot
```

## Vectorize the cluster assignments for training the LSTM

Our final step is to take the sequence of cluster assignments for each pitch contour and split it into into training, validation, and test sets for the LSTM modeling step. Intuitively, we want to generate "question and answer" sub-sequences where the question/input is the prior sequence of pitch shapes and the answer/target is the next pitch shape in the sequence.

First, I'm adding the number of segments (q-shapes in each clip), which will be useful for reconstructing the utterances from the training/test sequences)

```{r}
add_utt_duration_segs(d_final$d_clusters) -> d_final$d_clusters
```

Now we create the dataset. We want to be able to control the proportion of data used for training. We also want to be able to do experiments with the proportion of:

* ADS/CDS utterance types 
* male/female
* speaker (caregiver vs. not caregiver) [TODO]

To do this, we add the relevant knobs to our dataset generator function.

```{r generate lstm dataset}
d_lstm <- generate_lstm_dataset(d_final$d_clusters, 
                                max_seq_len = dnn_dataset_config$seq_max_len, 
                                skip = dnn_dataset_config$skip_val,
                                train_test_split = dnn_dataset_config$prop_train,
                                prop_cds = dnn_dataset_config$prop_train_cds)

# save coefs and cluster sequences
fst::write_fst(d_final$centers, here("data/03_summaries/lena-pred-kmeans-centers.fst"))
fst::write_fst(d_final$d_clusters, here("data/03_summaries/lena-pred-poly-coefs.fst"))
write_rds(d_lstm, here("data/03_summaries/lena-pred-lstm-train-test.rds"))
```
