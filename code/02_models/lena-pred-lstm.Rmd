---
title: "LSTM implementation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

## Set up LSTM language model

Neural language models are designed to learn how to predict the next word in a sequence given the prior context. In our case, words are cluster assignments for each 100-ms segment of pitch contours.

The implementation of the LSTM below is borrowed from this [tutorial] (https://keras.rstudio.com/articles/examples/lstm_text_generation.html) where they implement a character-level neural language model. 

```{r libraries}
library(keras); library(tfruns); library(cloudml)
library(here); library(tidyverse)
read_path <- "data/03_summaries"
```

## Read data and set up parameters

```{r read data}
# data
d <- read_rds(here(read_path, "lena-pred-lstm-seqs.rds"))

# globals
n_clusters <- d$next_cluster %>% unique() %>% length()
input_shape <- n_clusters + 1 # size of vocabulary
output_dim <- 30 
input_length <- d$prev_cluster_seq[[1]] %>% length()
lstm_units <- 30

# vectorize data
x_in <- d$prev_cluster_seq %>% unlist() %>% matrix(ncol = input_length, byrow = TRUE)
y_out <- d$next_cluster_one_hot %>% unlist() %>% matrix(ncol = n_clusters+1, byrow = TRUE)
```

## Model Definition 

```{r}
model <- keras_model_sequential()

model %>%
  layer_embedding(
    input_dim = input_shape, 
    output_dim = output_dim, 
    input_length = input_length) %>% 
  layer_lstm(units = lstm_units, input_shape = c(input_length, n_clusters+1)) %>%
  layer_dense(n_clusters+1, activation = "softmax")

optimizer <- optimizer_rmsprop(lr = 0.01)

model %>% compile(
  loss = "categorical_crossentropy", 
  optimizer = optimizer
)

summary(model)
```

## Training 

```{r}



```

Fit the model

```{r}
early_stop <- callback_early_stopping(monitor = "val_loss", min_delta = 0.0001,
                                      patience = 0, verbose = 0, mode = "auto")

history <- model %>% 
  fit(x_in, y_out,
      batch_size = 10,
      validation_split = 0.2,
      shuffle = TRUE,
      return_sequences = TRUE,
      epochs = 150,
      callbacks = early_stop
  )
```

Visualize the change in loss function during training.

```{r}
plot(history)
```

## Make predictions 

```{r}
preds <- model %>% predict()
```

## Evaluate the model

