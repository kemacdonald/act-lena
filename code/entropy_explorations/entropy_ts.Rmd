---
title: "Time Series Entropy Exploration"
author: "Kyle MacDonald"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```

## Set up

```{r, echo = F, warning=F, message=F}
library(pracma); library(TSEntropies); library(MSMVSampEn)
library(magrittr); library(tidyverse)
```

This document summaarises my explortion and experiments with a variety of meaures for estimating the entropy/uncertainty/information content of a time-series. 

## Why measure entropy of time series? 

Entropy provides a way to quantify the amount of regularity and unpredictability of fluctuations in a time-series. Regularity can be a useful wway to distinguish between two time series (or groups of time-series) that would otherwise be indistinguishable using moment statistics. From the Wiki page on Approximate Entropy,

```
For example, there are two series of data:

* series 1: (10,20,10,20,10,20,10,20,10,20,10,20...), which alternates 10 and 20.
* series 2: (10,10,20,10,20,20,20,10,10,20,10,20,20...), which has either a value of 10 or 20, chosen randomly, each with probability 1/2.

Moment statistics, such as mean and variance, will not distinguish between these two series. Nor will rank order statistics distinguish between these series. Yet series 1 is "perfectly regular"; knowing one term has the value of 20 enables one to predict with certainty that the next term will have the value of 10. Series 2 is randomly valued; knowing one term has the value of 20 gives no insight into what value the next term will have.
```

Time-series entropy measures have been used across a wide variety of data types, including cardiovascular, sleep, gait, and EEG signals. 

## How to measure the entropy of a time-series?

The goal of an entropy calcuation is to estimate regularity and predictability in some signal. Put another way, we aim to quanitfy the rate of information a process generates through time. We can compute the entropy of a time-series by slicing a single-dimensional time-series $X$ into a series of shorter vectors with length $m$ (the embedding dimension). Then we use an algorithm to iteratively compare the similarity of the embedded vectors at different time lags. Intuitively, if a time-series has more embedded vectors that are similar to one another, then it is more predictable and has lower entropy.

Over the last three decades, researchers have developed several measure of the entropy of time-series data. The primary methods are:

* Approximate Entropy (Pincus, 1991)
* Sample Entropy (Richman and Moorman, 2000)
* Multiscale Entropy (Costa et al., 2002)

These approaches build on one another, improving on the shortcomings of the prior measure. 

### Approximate entropy 

Pincus defined Approximate Entropy as:

$$ \mathrm {ApEn} =\Phi ^{m}(r) -\Phi ^{m+1}(r) $$
where $\Phi ^{m}(r)$ is: 

$${\displaystyle \Phi ^{m}(r)=(N-m+1)^{-1}\sum _{i=1}^{N-m+1}\log(C_{i}^{m}(r))}$$

$\log$ is the natural logarithm for $m$ and $r$. $r$ is a tolerance parameter that controls the threshold for defining two embedded vector as similar (criterion for similarity). $m$ controls the size of the embedding dimension. $\displaystyle C_{i}^{m}$ is defined as: 

$${\displaystyle C_{i}^{m}(r)=({\text{number of }}x(j){\text{ such that }}d[x(i),x(j)]\leq r)/(N-m+1)\,}$$
in which $d[x,x^{*}]$ is defined as:

$$d[x,x^{*}]=\max _{a}|u(a)-u^{*}(a)|$$
To get the inputs to this distance formula, we form a sequence of vectors $\mathbf {x} (1), {\displaystyle \mathbf {x} (2),\ldots ,\mathbf {x} (N-m+1)}$, in $\mathbf {R} ^{m}$, real $\ m$-dimensional space defined by $\mathbf {x} (i)=[u(i),u(i+1),\ldots ,u(i+m-1)]$.


### Sample entropy 

Sample entropy (SampEn) is a modification of approximate entropy with the main advantages of being less dependent on data length independence. Also, there is a small computational difference: In ApEn, the comparison between the template vector (see below) and the rest of the vectors also includes comparison with itself. These self-matches are not included in SampEn.

Sample Entropy is defined as:

$$SampEn=-\log {A \over B}$$
Where $A =$ number of template vector pairs having $d[X_{{m+1}}(i),X_{{m+1}}(j)]<r$ and $B =$ number of template vector pairs having $d[X_{m}(i),X_{m}(j)]<r$.

Similar to approximate entropy, $r$ is the criterion for similarity and $m$ controls the size of the embedding dimension.

Here is a visual represention of the sample entropy computation. Dotted horizontal lines around
data points u[1], u[2] and u[3] represent u[1] ± r, u[2] ± r, and u[3] ± r, respectively. Two data values match each other, that is, they are indistinguishable, if the absolute difference between them is ≤ rAll green points represent data points that match the data point u[1]. Similarly, all red and blue points match the data points u[2] and u[3], respectively. 

```{r, echo = F}
knitr::include_graphics(here::here('code/entropy_explorations/sampent_example.png'))
```

### Multiscale entropy

Multiscale entropy is an extension of approximate/sample entropy that examines the complexity of a time series at different time scales via a process of coarse-graining at different intervals specified by thee value of a $ \delta$ parameter.

The main advantage of multiscale entropy is the ability to detect time series which are locally unpredictable, but which exhibit "low frequency" structure and might show incre "scale invariant” in some sense, and should show constant entropy at any time scale.

We define multiscale entropy as: 

$$ SampEn\left(m,r,\delta \right)=-\log {A_{\delta } \over B_{\delta }} $$
where $\delta$ is a skipping parameter that defines the scale of coarse-grained time series. Specifically, for a given time series, multiple coarse-grained time series are
constructed by averaging the data points within non-overlapping windows of increasing length $\delta$. Each element of the coarse-grained time series, $x_j^{(\delta)}$, is calculated according to the equation: 

$$x_j^{(\delta)} = \frac{1}{\delta} \sum_{i = (j-1) \delta + 1} x_i$$
where $\delta$ represents the scale factor and $1 \leq j\leq N/\delta $. The length of each coarse-grained time series is $N/\delta$ . For $scale = 1$, the coarse-grained time series is simply the original time series.

Here is a figure showing the coarse-graining process:

```{r, echo = F}
knitr::include_graphics(here::here('code/entropy_explorations/mse_coarse.png'))
```

SampEn or ApEn is calculated for each coarse-grained time series, and then plotted as a function of the scale factor.

## Experiments

First we generate several, short time-series that vary in terms of their predictability or regularity: 

* Periodic (zero entropy, fully predictable)
* Sine (zero entropy, fully predictable)
* Pink Noise (higher entropy, predictable at longer timescales)
* White Noise (max entropy, impossible to predict)

```{r}
ts_samples <- 500

ts_periodic <- rep(61:65, ts_samples / 5) 
ts_sin <- sin(seq(1, 100, by = 100 / ts_samples)) 
ts_white_noise <- rnorm(ts_samples)
ts_pink_noise <- tuneR::noise(kind = c("pink"),
                              samp.rate = ts_samples) %>% 
  .@left

curves_list <- list(ts_periodic, ts_sin, ts_pink_noise, ts_white_noise)
```

Next, let's plot those curves along with their estimates of sample entropy using the default parameter settings  $r = 0.2*sd(ts)$ and $m = 2$.

```{r, fig.width=12, fig.asp=0.85, out.width = "75%"}
plot_ts_ent <- function(ts) {
  
  qplot(seq(1, length(ts)), ts, geom = 'line') +
    labs(x = "X", y = "Y", 
         subtitle = paste('sample entropy = ', 
                          sample_entropy(ts) %>% round(2)))
}

plot_labels <- c("periodic", "sine", "pink noise", "white noise")

curves_list %>% 
  purrr::map(plot_ts_ent) %>% 
  cowplot::plot_grid(plotlist = ., ncol = 1, scale = 0.85,
                     labels = plot_labels, hjust = .0001)
```

We can see that `sample_entropy()` provides a reasonable rank ordering of entropy for the various signals: $periodic < sine < pink \ noise < white \ noise$.






