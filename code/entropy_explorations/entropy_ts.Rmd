---
title: "Time Series Entropy Exploration"
author: "Kyle MacDonald"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F,
                      fig.align = 'center',
                      fig.width= 10, fig.asp=0.85, 
                      out.width = "75%")
```

## Setup

```{r, echo = F, warning=F, message=F}
set.seed(27)
library(pracma); library(TSEntropies); library(MSMVSampEn)
library(magrittr); library(tidyverse)
```

This document summaarises my explortion and experiments with a variety of meaures for estimating the entropy/uncertainty/information content of a time-series. 

## Why measure entropy of time series? 

Entropy provides a way to quantify the amount of regularity and unpredictability of fluctuations in a time-series. Regularity can be a useful wway to distinguish between two time series (or groups of time-series) that would otherwise be indistinguishable using moment statistics. From the Wiki page on Approximate Entropy,

```
For example, there are two series of data:

* series 1: (10,20,10,20,10,20,10,20,10,20,10,20...), which alternates 10 and 20.
* series 2: (10,10,20,10,20,20,20,10,10,20,10,20,20...), which has either a value of 10 or 20, chosen randomly, each with probability 1/2.

Moment statistics, such as mean and variance, will not distinguish between these two series. Nor will rank order statistics distinguish between these series. Yet series 1 is "perfectly regular"; knowing one term has the value of 20 enables one to predict with certainty that the next term will have the value of 10. Series 2 is randomly valued; knowing one term has the value of 20 gives no insight into what value the next term will have.
```

Time-series entropy measures have been used across a wide variety of data types, including cardiovascular, sleep, gait, and EEG signals. 

## How to measure the entropy of a time-series?

The goal of an entropy calcuation is to estimate regularity and predictability in some signal. Put another way, we aim to quanitfy the rate of information a process generates through time. We can compute the entropy of a time-series by slicing a single-dimensional time-series $X$ into a series of shorter vectors with length $m$ (the embedding dimension). Then we use an algorithm to iteratively compare the similarity of the embedded vectors at different time lags. Intuitively, if a time-series has more embedded vectors that are similar to one another, then it is more predictable and has lower entropy.

Over the last three decades, researchers have developed several measure of the entropy of time-series data. The primary methods are:

* Approximate Entropy (Pincus, 1991)
* Sample Entropy (Richman and Moorman, 2000)
* Multiscale Entropy (Costa et al., 2002)

These approaches build on one another, improving on the shortcomings of the prior measure. 

### Approximate entropy 

Pincus defined Approximate Entropy as:

$$ \mathrm {ApEn} =\Phi ^{m}(r) -\Phi ^{m+1}(r) $$
where $\Phi ^{m}(r)$ is: 

$${\displaystyle \Phi ^{m}(r)=(N-m+1)^{-1}\sum _{i=1}^{N-m+1}\log(C_{i}^{m}(r))}$$

$\log$ is the natural logarithm for $m$ and $r$. $r$ is a tolerance parameter that controls the threshold for defining two embedded vector as similar (criterion for similarity). $m$ controls the size of the embedding dimension. $\displaystyle C_{i}^{m}$ is defined as: 

$${\displaystyle C_{i}^{m}(r)=({\text{number of }}x(j){\text{ such that }}d[x(i),x(j)]\leq r)/(N-m+1)\,}$$
in which $d[x,x^{*}]$ is defined as:

$$d[x,x^{*}]=\max _{a}|u(a)-u^{*}(a)|$$
To get the inputs to this distance formula, we form a sequence of vectors $\mathbf {x} (1), {\displaystyle \mathbf {x} (2),\ldots ,\mathbf {x} (N-m+1)}$, in $\mathbf {R} ^{m}$, real $\ m$-dimensional space defined by $\mathbf {x} (i)=[u(i),u(i+1),\ldots ,u(i+m-1)]$.


### Sample entropy 

Sample entropy (SampEn) is a modification of approximate entropy with the main advantages of being less dependent on data length independence. Also, there is a small computational difference: In ApEn, the comparison between the template vector (see below) and the rest of the vectors also includes comparison with itself. These self-matches are not included in SampEn.

Sample Entropy is defined as:

$$SampEn=-\log {A \over B}$$
Where $A =$ number of template vector pairs having $d[X_{{m+1}}(i),X_{{m+1}}(j)]<r$ and $B =$ number of template vector pairs having $d[X_{m}(i),X_{m}(j)]<r$.

Similar to approximate entropy, $r$ is the criterion for similarity and $m$ controls the size of the embedding dimension.

Here is a visual represention of the sample entropy computation. Dotted horizontal lines around
data points u[1], u[2] and u[3] represent u[1] ± r, u[2] ± r, and u[3] ± r, respectively. Two data values match each other, that is, they are indistinguishable, if the absolute difference between them is ≤ rAll green points represent data points that match the data point u[1]. Similarly, all red and blue points match the data points u[2] and u[3], respectively. 

```{r, echo = F}
knitr::include_graphics(here::here('code/entropy_explorations/sampent_example.png'))
```

Therefore, in this case, the number of sequences matching the 2-component template sequences is two and the number of sequences matching the 3-component template sequence is 1. These calculations are repeated for the next 2-component and 3-component template sequence, which are, (u[2], u[3]) and (u[2], u[3], u[4]), respectively.

### Multiscale entropy

Multiscale entropy is an extension of approximate/sample entropy that examines the complexity of a time series at different time scales via a process of coarse-graining at different intervals specified by thee value of a $ \delta$ parameter.

The main advantage of multiscale entropy is the ability to detect time series which are locally unpredictable, but which exhibit "low frequency" structure and might show incre "scale invariant” in some sense, and should show constant entropy at any time scale.

We define multiscale entropy as: 

$$ SampEn\left(m,r,\delta \right)=-\log {A_{\delta } \over B_{\delta }} $$
where $\delta$ is a skipping parameter that defines the scale of coarse-grained time series. Specifically, for a given time series, multiple coarse-grained time series are
constructed by averaging the data points within non-overlapping windows of increasing length $\delta$. Each element of the coarse-grained time series, $x_j^{(\delta)}$, is calculated according to the equation: 

$$x_j^{(\delta)} = \frac{1}{\delta} \sum_{i = (j-1) \delta + 1} x_i$$
where $\delta$ represents the scale factor and $1 \leq j\leq N/\delta $. The length of each coarse-grained time series is $N/\delta$ . For $scale = 1$, the coarse-grained time series is simply the original time series.

Here is a figure showing the coarse-graining process:

```{r, echo = F}
knitr::include_graphics(here::here('code/entropy_explorations/mse_coarse.png'))
```

SampEn or ApEn is calculated for each coarse-grained time series, and then plotted as a function of the scale factor.

## Experiments

First we generate several, short time-series that vary in terms of their predictability or regularity: 

* Periodic (zero entropy, fully predictable)
* Sine (zero entropy, fully predictable)
* Pink Noise (higher entropy, predictable at longer timescales)
* White Noise (max entropy, impossible to predict)

```{r}
ts_samples <- 500

ts_periodic <- rep(61:65, ts_samples / 5) 
ts_sin <- sin(seq(1, 100, by = 100 / ts_samples)) 
ts_white_noise <- rnorm(ts_samples)
ts_pink_noise <- tuneR::noise(kind = c("pink"),
                              samp.rate = ts_samples) %>% 
  .@left

curves_list <- list(ts_periodic, ts_sin, ts_pink_noise, ts_white_noise)
names(curves_list) <- c("periodic", "sine", "pink noise", "white noise")
```

Next, let's plot those curves along with their estimates of sample entropy using the default parameter settings  $r = 0.2*sd(ts)$ and $m = 2$.

```{r}
plot_ts_ent <- function(ts) {
  
  qplot(seq(1, length(ts)), ts, geom = 'line') +
    labs(x = "X", y = "Y", 
         subtitle = paste('sample entropy = ', 
                          sample_entropy(ts) %>% round(2)))
}

curves_list %>% 
  purrr::map(plot_ts_ent) %>% 
  cowplot::plot_grid(plotlist = ., ncol = 1, scale = 0.85,
                     labels = names(.), hjust = .0001)
```

We can see that `sample_entropy()` provides a reasonable rank ordering of entropy for the various signals: $periodic < sine < pink \ noise < white \ noise$.

### Why does sine wave not also have zero entropy?

Ack, don't have a precise answer. But I found other papers where they show similar entropy values for sine waves. From Lee et al. (2013):

> We computed ApEn values for six regular sine curve sequences of 4000 points, with a simulated sampling rate of 250 Hz. The sine frequencies used were 1, 2, 4, 8, 16, and 32 Hz ApEn values for the simulated data ranged between 0.07 and 0.29 for the sine curves.

### What happens when you change tolerance?

Let's simulate the entropy calculation for different levels of $r$ and plot.

```{r}
sim_r <- function(ts, r_vals) {
  r_vals %>% 
    purrr::map_dbl(function(x) sample_entropy(ts = ts, r = x * sd(ts)))
}

r_vals <-  seq(0.1, 4, by = 0.25) 
r_samp_ents <- curves_list %>% purrr::map(sim_r, r_vals)

r_plots <- r_samp_ents %>% 
  purrr::map(~ qplot(x = r_vals,  y = .x, geom = c('line', 'point')) + 
               labs(x = 'tolerance value (r)', y = 'sample entropy') +
               lims(x = c(0, 4), y = c(0,3.2)), 
             r_vals)

cowplot::plot_grid(plotlist = r_plots, labels = names(r_samp_ents),
                   vjust = 0.1)
```

As the tolerance increases, the amount of entropy or uncertainty decreases. This makes sense since the probability of getting similar vectors has increased meaning the time-series is more regular. 

### What happens when you change the embedding size?

Here we show what happens to the sample entropy calculation for the different time-series when you change the embedding size. 

```{r}
sim_m <- function(ts, m_vals) {
  m_vals %>% 
    purrr::map_dbl(function(x) sample_entropy(ts = ts, edim = x))
}

m_vals <- seq(2, 6, by = 1)
m_samp_ents <- curves_list %>% purrr::map(sim_m, m_vals)

m_plots <- m_samp_ents %>% 
  purrr::map(~ qplot(x = m_vals,  y = .x, geom = c('line', 'point')) + 
               labs(x = 'embedding size (m)', y = 'sample entropy') +
                lims(y = c(0, 3.5)), 
             m_vals)

cowplot::plot_grid(plotlist = m_plots, labels = names(m_samp_ents),
                   vjust = 0.2)
```

As you increase the embedding dimension, 

### What happens when you vary the length of the time-series?

TODO: 




