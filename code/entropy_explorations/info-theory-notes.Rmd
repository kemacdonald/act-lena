---
title: "Information theory notes"
author: "Kyle MacDonald"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(magrittr)
library(tidyverse)
```

## Twenty questions game

The game is played as follows: 

> The first player (the “adult”) in this two-player game thinks of something, and by a series of yes-no questions, the other player (the “child”) attempts to guess what it is.

If we describe your opponent by the list of probabilities he has of choosing any particular option, we can then talk about the average number of yes/no questions it takes before the game is finished.

  * $N$ words, with the label $x_i$
  * game will end in predetermined number of steps $L(x_i)$
  
We can measure the performance of a set of y/n questions by taking the sum over all the possible $L(x_i)$ multiplied the probability  $P(x_i)$: 

$$Script \ performance = \sum_{i=1}^{N}P(x_i)L(x_i)$$

We can now ask: what is the optimal set of y/n questions? The fastest to finish the game? 

$$H(x) = -\sum_{i=1}^{N}P(x_i)log_{2}P(x_i)$$
In R code, we can make a function that takes a vector of probabilities and computes the entropy of a distribution:

```{r}
compute_entropy <- function(prob_vect) {
  prob_vect %>% purrr::map_dbl(~ .x * log2(.x)) %>% sum() * -1
}
```

```{r}
probs <- list(0.25, 0.25, 0.5)
compute_entropy(probs)
```

> H(X) goes by a number of different names: “uncertainty”, “information”, even “entropy” (a term from the physical sciences, which we’ll return to later).

This quantity allows us to talk about how uncertain we are about the outcome of any probability distribution. 

## Is H(x) a subjective or objective quantity? 

> H(X) is a fundamentally epistemic quantity. It quantifies how an actual agent in the world goes about gathering information about what is going on. ... information in this subjective sense corresponds directly to a basic quantity in the physical sciences, entropy. Entropy is a fact about the world; you can look up, for example, the “entropy released when ice melts into water”. How is this possible?

## Encoding and memory

> Huffman Encoding. Most basic form of compression/encoding. It is an incredibly simple way to achieve optimal transmission and storage, and it’s therefore widespread in the computer world. It takes advantage of the biases in the source, giving nicknames or shortcuts to the most common things you say 

> The outcome of a process with a lot of uncertainty is harder to remember, or transmit; conversely, an agent with a little experience can develop a codebook that allows them to efficiently gather and store information about the world

## Coarse-graining

Shannon's goal: *We want a function, in other words, call it $H({p})$, that takes a list of probabilities and spits out a single number, uncertainty.* Let’s require the function to obey four simple axioms:

  1. Continuity (if I only change the probabilities a little, the information of the process should change only a little).
  2. Symmetry (if I reorder the list of probabilities I gave you, you should get the same answer).
  3. Condition of Maximum Information: $H(p)$ is at its maximum value when all the $p_i$ are equal.
  4. Coarse-Graining (discussed below)
  
Example of coarse-graining: 

> When we say “he was driving a car” rather than “he was driving a red Chevrolet with Massachusetts plates”, we’re coarse-graining, ignoring, or refusing to transmit, a bunch of information that would distinguish very different events.

## Coding failure, Cognitive Surprise, and Kullback-Leibler Divergence

Kullback-Leibler divergence quantifies coding failure or a failure of expectations: 

$$KL(p|q) = \sum_{i=1}^N q(x_i) log_2 \frac{q(x_i)}{p(x_i)}$$

where p(x) is the distribution you trained on but q(x) is the new distribution you’re now encountering.

We can talk, for example, about the “surprise” (or “Bayesian surprise”) of one distribution given that you’re expecting another.

> The first use of KL divergence to quantify surprise, or coding failure, in a cognitive system appears to be Itti & Baldi’s work on eye-tracking. They did something very clever: they took a movie clip, and measured the KL divergence spatially over the screen. Formally, they divided up the screen into little patches. In each patch, they computed the probability distribution over pixel colors at time t (that’s the p distribution) and then again at time t plus one second (that’s the q distribution). Then they could compute the KL divergence between p and q in each patch, and (since the movie clip lasts more than a few seconds), actually see how it evolves over time. Amazingly, their eye-tracking devices showed that people tended to look not (say) at the bright patches, or the uncertain patches, but at the high KL-divergence patches.

## Einstein and Cromwell's rule

KL-divergence is not symmetric quantity! So we have to be careful using it as distance metric.

 > .An organism that grows up in a rich sensory environment will, in general, be less surprised on encountering an improvised one than an organism who has grown up in an impoverished environment and encounters a rich one.

## Mutual information 

Once you can measure H(X), the uncertainty in a process, you can talk about how that uncertainty changes when you acquire information from somewhere else. For example,

> You might be maximally uncertain about the card on the top of a shuffled deck, your uncertainty goes down a great deal once I tell you the suit. Instead of having an equal choice over 52 cards (an entropy of log2 52, about 5.7 bits), you have an equal choice over the thirteen cards of that suit (log2 13, or 3.7, bits). Put another way, telling you the suit was worth two bits of information.

What does knowing $y$ tell you about the probability distribution of X? Formally, we compute the conditional entropy of X given Y:  

$$H(X|Y) = \sum_{j \in M} H(X|y_j) P(y_j)$$
where we assume that there are M possible values of Y.

However, it can be shown (using something called Jensen’s inequality) that on average, information never hurts. In other words, H(X|Y ) is always less than, or (in the case that Y and X are independent) equal to H(X).

If we want to talk about the extent to which Y reduces our uncertainty in X, then we can cmopute the mutual information:

$$I(X,Y) = H(X) - H(X|Y)$$

You can think of it as the “most general” form of correlation coefficient that you can measure.

## Jensen-Shannon Distance

This is called the Jensen-Shannon Distance, or JSD, between P and Q:

$$JSD(P,Q) = H(M) - \frac{1}{2} (H(P) - H(Q))$$
where M is the mixture distribution of the two adults, $m_i$ is equal to $\frac{1}{2}(p_i +q_i)$.

> JSD tells you how much one sample, on average serves to distinguish between the two possibilities. Once you know which distribution you’re drawing from you will (on average) be able to construct a more efficient question tree.

## Measuring information

You begin by estimating the probability of events from frequency counts; given these estimated probabilities, it’s a simple matter to plug them in to the formula you care about—whether it be uncertainty, Kullback-Leibler divergence, mutual information, or Jensen-Shannon Distance.

Information theory tells you what you know, but it doesn’t tell you what matters; you need something like a utility function from the decision theorists or game theories to tell you about that

New, often very large, data-sets give us the sample sizes we need to test sophisticated hypotheses about what underlying mechanisms might be tracked by quantities such as KL, JSD, and Mutual Information.